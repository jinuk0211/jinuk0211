ì—°ë½ì²˜
---------------
- https://www.threads.net/@rien_n_est
- ğŸ“« dnr9333@gmail.com



ê¸°ìˆ ìŠ¤íƒ 
----------------
#<img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&logo=#PyTorch&logoColor=D1180B"/>
<img src="https://img.shields.io/badge/C++-345F53?style=flat-square&logo=C++&logoColor=345F53"/>
<img
src="https://img.shields.io/badge/FASTAPI-43EED6?style=flat-square&logo=#009688&logoColor=43EED6"/>
<img src="https://img.shields.io/badge/Django-0A3711?style=flat-square&logo=#Django&logoColor=0A3711"/>
<img src="https://img.shields.io/badge/Flutter-99CCFF?style=flat-square&logo=#Flutter&logoColor=99CCFF"/>
<img src="https://img.shields.io/badge/Docker-00q9F4?style=flat-square&logo=#Docker&logoColor=0019F4"/>









ëŒ€ì™¸í™œë™
---------------------

dacon ê³ ê° ëŒ€ì¶œë“±ê¸‰ ë¶„ë¥˜ ê²½ì§„ëŒ€íšŒ - ìƒìœ„ 15%

dacon ì§ì˜í¼ì¦ ai ê²½ì§„ëŒ€íšŒ - ìƒìœ„ 10%

cpp ì½”ë“œ ìœ ì‚¬ì„± íŒë‹¨ ai ê²½ì§„ëŒ€íšŒ - ìƒìœ„ 8%

ìºê¸€ LLM science exam -bronze medal 

ìºê¸€ LLM - Detect AI Generated Text -bronze medal

í† ìµ 960,

ì •ë³´ì²˜ë¦¬ì‚°ì—…ê¸°ì‚¬ 

azure microsoft AI a1-900

ë° aws certified cloud parctitioner ìê²©ì¦ 

Improved generative MCTS verifier with self critique ë…¼ë¬¸ 1ì €ì
[COLING2025] 

enhancing visual understanding capability of multimodal model through inference scaling ì œ1ì €ì - ëŒ€í•œì „ìê³µí•™íšŒ

SSM ëª¨ë¸ í™œìš©í•œ ecgë°ì´í„° ì˜ˆì¸¡ ì œ1ì €ì - ì •ë³´í†µì‹ í•™íšŒ ì¶˜ê³„ìš°ìˆ˜ë…¼ë¬¸ ì„ ì •


AI ë…¼ë¬¸ ë¦¬ë·° 
-------------
<https://www.threads.net/@rien_n_est>



![jinuk's GitHub stats](https://github-readme-stats.vercel.app/api?username=jinuk0211&show_icons=true&theme=radical)
![junuk's GitHub stats](https://github-readme-stats.vercel.app/api?username=jinuk0211&show_icons=true&theme=radical)





attention is all we need
-----------
```python
class Attention(nn.Module):
    """attention is all you need"""

    def __init__(self, config, block_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.hidden_size = config.d_model
        self.num_heads = config.n_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.max_position_embeddings = config.max_seq_len
        self.block_idx = block_idx
        if block_idx is None:
            logger.warning_once(
                 block_idxë¥¼ ì œê³µí•˜ì§€ ì•Šê³  {self.class.name}ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ê²ƒì€ ê¶Œì¥ë˜ì§€ ì•Šìœ¼ë©°,
                 cachingì„ ì‚¬ìš©í•  ê²½ìš° forwardê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                 attention class ìƒì„±ì‹œ block id ë¥¼ ê¼­ ì…ë ¥í•´ì£¼ì„¸ìš”.
            )

        attn_config = config.attn_config
        self.attn_pdrop = attn_config.attn_pdrop
        self.clip_qkv = attn_config.clip_qkv
        self.num_key_value_heads = attn_config.kv_n_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.rope_theta = attn_config.rope_theta
        self.is_causal = True

        self.Wqkv = nn.Linear(
            self.hidden_size, self.hidden_size + 2 * self.num_key_value_heads * self.head_dim, bias=False
        )
        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
        self.rotary_emb = RotaryEmbedding(
            self.head_dim,
            max_position_embeddings=self.max_position_embeddings,
            base=self.rope_theta,
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_ids: torch.LongTensor,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:
        bsz, q_len, _ = hidden_states.size() #batchsize

        qkv_states = self.Wqkv(hidden_states)
        min_val = -self.clip_qkv if self.clip_qkv is not None else None
        max_val = self.clip_qkv
        qkv_states = qkv_states.clamp(min=min_val, max=max_val)

        query_states, key_states, value_states = qkv_states.split(
            [
                self.hidden_size,
                self.num_key_value_heads * self.head_dim,
                self.num_key_value_heads * self.head_dim,
            ],
            dim=2,
        )

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        cos, sin = self.rotary_emb(value_states, position_ids)
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.block_idx, cache_kwargs)

        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attention_mask is not None:  #lengthì— ê´€ê³„ì—†ì´ 
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights = attn_weights + causal_mask

        # fp32ë¡œ upcast
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attn_pdrop, training=self.training)
        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` ì˜ sizeëŠ” {(bsz, self.num_heads, q_len, self.head_dim)}ì—¬ì•¼í•©ë‹ˆë‹¤ , í•˜ì§€ë§Œ ì§€ê¸ˆ size :"
                + f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
        attn_output = self.out_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value

```
